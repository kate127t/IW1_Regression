{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MyLinearRegression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MyLinearRegression.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class myLinearRegression:\n",
    "    def __init__(self, standardize=False, \n",
    "                 learning_rate=0.01, \n",
    "                 num_iters=1000,\n",
    "                 tol=1e-4,\n",
    "                 print_J=False):\n",
    "        \"\"\"Initialize Linear Regression.\n",
    "        \n",
    "        Args:\n",
    "            standardize (bool): Whether to standardize the data.\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            max_iter (int): Maximum number of iterations for gradient descent.\n",
    "            tol (float): Tolerance for gradient descent.\n",
    "            print_J (bool): Whether to print cost at each 100th iteration.\n",
    "        \"\"\"\n",
    "        self.standardize = standardize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.tol = tol\n",
    "        self.print_J = print_J\n",
    "\n",
    "    def normalize(self,X):\n",
    "        '''\n",
    "        Нормалізує датасет з характеристиками\n",
    "        \n",
    "        Параметри:\n",
    "        X - набір характеристик\n",
    "        \n",
    "        Результат:\n",
    "        X_new - набір нормалізованих характеристик, (X-mean)/std\n",
    "        mean - вектор середніх значень характеристик\n",
    "        std - вектор стандартних девіацій характеристик\n",
    "        '''\n",
    "        mean = X.mean(axis = 0)\n",
    "        std = X.std(axis = 0)\n",
    "        X_new  = np.divide(np.subtract(X,mean),std)\n",
    "        return X_new, mean, std\n",
    "    \n",
    "    def prepare_X(self,X):\n",
    "        '''\n",
    "        Формує датасет з рисами, де першою колонкою буде колонка з одиницями.\n",
    "        \n",
    "        Параметри:\n",
    "        X - вхідний датасет з прикладами, по одному в рядок. Кожна риса - відповідна колонка.\n",
    "        \n",
    "        Результат:\n",
    "        X_new - датасет, який складається з колонки одиниць, а решта колонок з X    \n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        ones = np.ones((m, 1))\n",
    "        X_new = np.array(X[:])\n",
    "        X_new = np.column_stack((ones, X_new))\n",
    "\n",
    "        return X_new\n",
    "    \n",
    "    def hypothesis(self,X, theta):\n",
    "        '''\n",
    "        Обчислює значення передбачуваної величини для вхідної матриці X та вектора параметрів thetha.\n",
    "        \n",
    "        Параметри:\n",
    "        X - матриця з рисами. Перша колонка - одиниці. Друга - дані риси.\n",
    "        thetha - вектор параметрів: [thetha0, thetha1]\n",
    "        \n",
    "        Результат:\n",
    "        Матриця значень шуканої величини для прикладів з X\n",
    "        '''\n",
    "        h_theta = np.dot(X, np.transpose(theta))\n",
    "        \n",
    "        return h_theta\n",
    "    \n",
    "    def cost_function(self,X, y, theta):\n",
    "        '''\n",
    "        Функція для обчислення штрафної функції J.\n",
    "        \n",
    "        Параметри:\n",
    "        X - тренувальний датасет. 0 - колонка з одиниць, далі - реальні риси\n",
    "        y - точні значення передбачуваної величини\n",
    "        thethe - вектор параметрів регресії\n",
    "        \n",
    "        Результат:\n",
    "        Дійсне число - значення штрафної функції для набору прикладів X та параметрів thetha\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        if m == 0:\n",
    "            return None\n",
    "        \n",
    "        J = 1/(2*m) * (np.sum(np.power (self.hypothesis(X,theta) - y,2),axis = 0))\n",
    "\n",
    "        return J\n",
    "    \n",
    "    def derivative(self,X, y, theta):\n",
    "        m = X.shape[0]\n",
    "        '''\n",
    "        Функція для обчислення похідних штрафної функції J по thetha.\n",
    "        \n",
    "        Параметри:\n",
    "        X - тренувальний датасет. 0 - колонка з одиниць, далі - реальні риси\n",
    "        y - точні значення передбачуваної величини\n",
    "        thetha - вектор параметрів регресії\n",
    "        \n",
    "        Результат:\n",
    "        Вектор похідних d_thetha\n",
    "        '''\n",
    "        \n",
    "        d_theta = 1/(m) * (np.dot ( np.transpose(X),(self.hypothesis(X,theta) - y)))\n",
    "\n",
    "        return d_theta\n",
    "    \n",
    "    def gradient_descent(self,X, y, theta):\n",
    "        '''\n",
    "        Функція, що реалізує градієнтний спуск для метода лінійної регресії.\n",
    "        \n",
    "        Параметри:\n",
    "        X - тренувальний датасет. 0 - колонка з одиниць, далі - реальні риси\n",
    "        y - точні значення передбачуваної величини\n",
    "        thetha - вектор початкових параметрів регресії\n",
    "        alpha - швидкість навчання\n",
    "        num_iters - кількість ітерацій\n",
    "        print_J - виведення штрафної функції на екран після кожної ітерації\n",
    "        \n",
    "        Результат:\n",
    "        theta - оптимальні значення параметрів регресії\n",
    "        J_history - масив історичних значень штрафної функції після кожної ітерації\n",
    "        \n",
    "        \n",
    "        1) J_i (theta_0, theta_1)\n",
    "        2)  theta_0 = theta_0 - alpha*dtheta_0\n",
    "            theta_1 = theta_1 - alpha*dtheta_1\n",
    "            |J_i-J_{i-1}| < eps || num_iters>10000000000 -> break\n",
    "        3) goto 1\n",
    "        '''\n",
    "        #X = self.prepare_X(X)\n",
    "        m = X.shape[0]\n",
    "        J_history = []\n",
    "        J = self.cost_function(X, y, theta)\n",
    "        if self.print_J == True:\n",
    "            print(J)\n",
    "        J_history.append(J)\n",
    "        for i in range(self.num_iters):\n",
    "            delta = self.derivative(X, y, theta)\n",
    "            theta = np.subtract(theta,np.dot(self.learning_rate,delta)) \n",
    "            J = self.cost_function(X, y, theta)\n",
    "            \n",
    "            if self.print_J == True:\n",
    "                print(J)\n",
    "            J_history.append(J)\n",
    "        return theta, J_history\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "\n",
    "        Args:\n",
    "            X (array): Data.\n",
    "            y (array): Target.\"\"\"\n",
    "        if self.standardize:\n",
    "            X_new, self.mean, self.std = self.normalize(X)\n",
    "        X_new = self.prepare_X(X)\n",
    "\n",
    "        self.theta = np.zeros(X_new.shape[1])\n",
    "        self.theta, self.costs = self.gradient_descent(X_new, y, self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the target.\n",
    "\n",
    "        Args:\n",
    "            X (array): Data.\n",
    "\n",
    "        Returns:\n",
    "            array: Predicted target.\"\"\"\n",
    "        if self.standardize:\n",
    "            X_new = np.divide(np.subtract(X,self.mean),self.std)\n",
    "        X_new = self.prepare_X(X)\n",
    "        y_pred = self.hypothesis(X_new,self.theta)\n",
    "        \n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Linear Regression)",
   "language": "python",
   "name": "linear_regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
